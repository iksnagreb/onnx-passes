# Demonstration of Convolution Lowering
This demo model comprises a single convolution layer which will be lowered into
a matrix multiplication. As convolution offers many configuration options, the
demo model can be regenerated by running the `export.py` to explore different
behaviors (in particular related to the grouping option). The pre-exported model
is configure in a simple MNIST-like configuration.
```bash
netron --browse model.onnx
```

## Lowering Convolution without Streamlining
Convolution is replaced by an input generator (`Im2Col`)  followed by a `MatMul`
and the optional bias extracted into a standalone `Add` operator. The `Im2Col`
can be expressed in pure ONNX as a `Gather` operator on the flattened input
tensor with precomputed indices for the sliding windows. Padding is extracted
into a standalone `Pad` operator at the input. While `Conv` operates in
channels-first layout, the lowered pattern is transposed into channels-last
layout to align with the `MatMul` axes.
```bash
onnx-passes -c cfg.yaml -o out.onnx model.onnx shape-inference fold-constants \
 lower-conv shape-inference fold-constants checker verify
netron --browse out.onnx
```
